#!/usr/bin/env python3
import argparse
import json
import os
from pathlib import Path

HARDWARE_MAP = {
    "little": "ProcessorType::kLittleCore",
    "medium": "ProcessorType::kMediumCore",
    "big": "ProcessorType::kBigCore",
    "gpu": "ProcessorType::kGPU",
}


def generate_header(schedule_obj: dict) -> str:
    schedule_id = schedule_obj["schedule_id"]
    device_id = schedule_obj["device_id"]
    chunks = schedule_obj["chunks"]

    # Derive sub-schedule name
    if schedule_id.startswith(device_id + "_"):
        sub_schedule_id = schedule_id[len(device_id) + 1 :]
    else:
        sub_schedule_id = schedule_id

    device_ns = f"device_{device_id}"
    schedule_ns = sub_schedule_id

    lines = []
    lines.append(f"// Auto-generated code for schedule: {schedule_id}")
    lines.append(f"// Device ID: {device_id}")
    lines.append("")
    lines.append("#pragma once")
    lines.append("")
    lines.append("#include <vector>")
    lines.append('#include "../task.hpp"')
    lines.append("#include <concurrentqueue.h>")
    lines.append("")
    lines.append(f"namespace {device_ns} {{")
    lines.append(f"namespace {schedule_ns} {{")
    lines.append("")
    lines.append(f'constexpr const char* kScheduleId = "{schedule_id}";')
    lines.append("")

    num_chunks = len(chunks)
    for i, chunk in enumerate(chunks):
        chunk_name = chunk["name"]
        if i == 0:
            lines.append(
                f"void stage_group_{chunk_name}(std::vector<Task>& in_tasks, moodycamel::ConcurrentQueue<Task>& out_q);"
            )
        elif i == num_chunks - 1:
            lines.append(
                f"void stage_group_{chunk_name}(moodycamel::ConcurrentQueue<Task>& in_q, std::vector<Task>& out_tasks);"
            )
        else:
            lines.append(
                f"void stage_group_{chunk_name}(moodycamel::ConcurrentQueue<Task>& in_q, moodycamel::ConcurrentQueue<Task>& out_q);"
            )

    lines.append("")
    lines.append(
        "void run_pipeline(std::vector<Task>& tasks, std::vector<Task>& out_tasks);"
    )
    lines.append("")
    lines.append(f"}}  // namespace {schedule_ns}")
    lines.append(f"}}  // namespace {device_ns}")
    lines.append("")
    return "\n".join(lines)


def generate_source(schedule_obj: dict, header_filename: str) -> str:
    schedule_id = schedule_obj["schedule_id"]
    device_id = schedule_obj["device_id"]
    chunks = schedule_obj["chunks"]

    if schedule_id.startswith(device_id + "_"):
        sub_schedule_id = schedule_id[len(device_id) + 1 :]
    else:
        sub_schedule_id = schedule_id

    device_ns = f"device_{device_id}"
    schedule_ns = sub_schedule_id

    lines = []
    lines.append(f"// Auto-generated code for schedule: {schedule_id}")
    lines.append(f"// Device ID: {device_id}")
    lines.append("")
    lines.append(f'#include "{header_filename}"')
    lines.append("")
    lines.append("#include <atomic>")
    lines.append("#include <thread>")
    lines.append('#include "../run_stages.hpp"')
    lines.append("")
    lines.append(f"namespace {device_ns} {{")
    lines.append(f"namespace {schedule_ns} {{")
    lines.append("")
    lines.append("static std::atomic<int> tasks_in_flight{0};")
    lines.append("static std::atomic<bool> done(false);")
    lines.append("")

    num_chunks = len(chunks)
    for i, chunk in enumerate(chunks):
        chunk_name = chunk["name"]
        hw_str = chunk["hardware"].lower()
        threads = chunk["threads"]
        stages = chunk["stages"]
        start_stage = stages[0]
        end_stage = stages[-1]

        if hw_str == "gpu":
            run_call = f"run_gpu_stages<{start_stage}, {end_stage}>(task.app_data);"
        else:
            pt_enum = HARDWARE_MAP.get(hw_str, "ProcessorType::kUnknown")
            run_call = f"run_stages<{start_stage}, {end_stage}, {pt_enum}, {threads}>(task.app_data);"

        if i == 0:
            # First chunk: increment tasks_in_flight
            lines.append(
                f"void stage_group_{chunk_name}(std::vector<Task>& in_tasks, moodycamel::ConcurrentQueue<Task>& out_q) {{"
            )
            lines.append("  for (auto& task : in_tasks) {")
            lines.append(f"    {run_call}")
            lines.append("    tasks_in_flight.fetch_add(1, std::memory_order_relaxed);")
            lines.append("    out_q.enqueue(task);")
            lines.append("  }")
            lines.append("}")
            lines.append("")
        elif i == num_chunks - 1:
            # Last chunk: decrement tasks_in_flight. If 0 => done=true
            lines.append(
                f"void stage_group_{chunk_name}(moodycamel::ConcurrentQueue<Task>& in_q, std::vector<Task>& out_tasks) {{"
            )
            lines.append("  while (!done.load(std::memory_order_acquire)) {")
            lines.append("    Task task;")
            lines.append("    if (in_q.try_dequeue(task)) {")
            lines.append(f"      {run_call}")
            lines.append("      out_tasks.push_back(task);")
            lines.append(
                "      int r = tasks_in_flight.fetch_sub(1, std::memory_order_relaxed) - 1;"
            )
            lines.append(
                "      if (r == 0) done.store(true, std::memory_order_release);"
            )
            lines.append("    } else {")
            lines.append("      std::this_thread::yield();")
            lines.append("    }")
            lines.append("  }")
            lines.append("  while (true) {")
            lines.append("    Task task;")
            lines.append("    if (!in_q.try_dequeue(task)) break;")
            lines.append(f"    {run_call}")
            lines.append("    out_tasks.push_back(task);")
            lines.append(
                "    int r = tasks_in_flight.fetch_sub(1, std::memory_order_relaxed) - 1;"
            )
            lines.append("    if (r == 0) done.store(true, std::memory_order_release);")
            lines.append("  }")
            lines.append("}")
            lines.append("")
        else:
            # Intermediate chunks: no inc/dec
            lines.append(
                f"void stage_group_{chunk_name}(moodycamel::ConcurrentQueue<Task>& in_q, moodycamel::ConcurrentQueue<Task>& out_q) {{"
            )
            lines.append("  while (!done.load(std::memory_order_acquire)) {")
            lines.append("    Task task;")
            lines.append("    if (in_q.try_dequeue(task)) {")
            lines.append(f"      {run_call}")
            lines.append("      out_q.enqueue(task);")
            lines.append("    } else {")
            lines.append("      std::this_thread::yield();")
            lines.append("    }")
            lines.append("  }")
            lines.append("  while (true) {")
            lines.append("    Task task;")
            lines.append("    if (!in_q.try_dequeue(task)) break;")
            lines.append(f"    {run_call}")
            lines.append("    out_q.enqueue(task);")
            lines.append("  }")
            lines.append("}")
            lines.append("")

    lines.append(
        "void run_pipeline(std::vector<Task>& tasks, std::vector<Task>& out_tasks) {"
    )
    for i in range(num_chunks - 1):
        lines.append(f"  moodycamel::ConcurrentQueue<Task> q_{i}{i+1};")
    lines.append("")
    lines.append("  tasks_in_flight.store(0, std::memory_order_relaxed);")
    lines.append("  done.store(false, std::memory_order_relaxed);")
    lines.append("")

    thread_names = []
    for i, chunk in enumerate(chunks):
        chunk_name = chunk["name"]
        tvar = f"t_{chunk_name}"
        thread_names.append(tvar)
        if i == 0:
            if num_chunks > 1:
                lines.append(
                    f"  std::thread {tvar}(stage_group_{chunk_name}, std::ref(tasks), std::ref(q_0{1}));"
                )
            else:
                lines.append(
                    f"  std::thread {tvar}(stage_group_{chunk_name}, std::ref(tasks), std::ref(out_tasks));"
                )
        elif i == num_chunks - 1:
            lines.append(
                f"  std::thread {tvar}(stage_group_{chunk_name}, std::ref(q_{i-1}{i}), std::ref(out_tasks));"
            )
        else:
            lines.append(
                f"  std::thread {tvar}(stage_group_{chunk_name}, std::ref(q_{i-1}{i}), std::ref(q_{i}{i+1}));"
            )

    lines.append("")
    for tvar in thread_names:
        lines.append(f"  {tvar}.join();")
    lines.append("}")
    lines.append("")
    lines.append(f"}}  // end namespace {schedule_ns}")
    lines.append(f"}}  // end namespace {device_ns}")
    lines.append("")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--in_dir", required=True, help="Directory with .json schedule files"
    )
    parser.add_argument(
        "--out_dir", required=True, help="Directory to output .hpp/.cpp files"
    )
    args = parser.parse_args()

    in_dir = Path(args.in_dir)
    out_dir = Path(args.out_dir)
    if not in_dir.is_dir():
        print(f"Error: input directory {in_dir} not found.")
        return
    out_dir.mkdir(parents=True, exist_ok=True)

    for json_file in in_dir.glob("*.json"):
        with open(json_file, "r") as f:
            data = json.load(f)
        if "schedule" not in data:
            print(f"Skipping {json_file}: no 'schedule' key found.")
            continue
        schedule_obj = data["schedule"]
        sid = schedule_obj["schedule_id"]
        base_name = sid

        hpp_name = f"{base_name}.hpp"
        cpp_name = f"{base_name}.cpp"

        hpp_content = generate_header(schedule_obj)
        cpp_content = generate_source(schedule_obj, hpp_name)

        with open(out_dir / hpp_name, "w") as hf:
            hf.write(hpp_content)
        with open(out_dir / cpp_name, "w") as cf:
            cf.write(cpp_content)

        print(f"Wrote {hpp_name} and {cpp_name} for {sid}")


if __name__ == "__main__":
    main()
